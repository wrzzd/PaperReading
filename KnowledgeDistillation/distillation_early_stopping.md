## Distillation $\approx$ Early Stopping ?
Paper Link: [Distillation â‰ˆ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network](https://arxiv.org/abs/1910.01255)

* Dark Knowledge: the extra information provided by the soft labels generated by the pre-trained teacher network.
    * The sematic similarity between different classes is part of the Dark Knowledge;
    * Help to refine noisy labels;
    * Represents the discrepancy of convergence speed of different types of information during the training of the neural network;
    * Neural network tends to fit informative information, like simple pattern, faster than non-informative and unwanted information such as noise. This paper calls this effect Anisotropic Information Retrieval (**AIR**)

* Bias from the eigenspace corresponding to the larger eigenvalues is reduced quicker by the gradient descent.
